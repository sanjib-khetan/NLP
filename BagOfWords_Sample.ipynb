{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a28a41d-5040-48ed-b118-b6ddff14dcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import string\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30b59ec1-30aa-47ad-94e5-76601fd5b90e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['I did not get what I was promised.',\n",
       "  'My package has not arrived yet.',\n",
       "  'No one in your team has been able to solve my problem.',\n",
       "  'I was put on hold for an hour!',\n",
       "  'Why is this so hard to use?',\n",
       "  'Why does it fail so easily?',\n",
       "  'Amazing customer service.',\n",
       "  'Love it.',\n",
       "  'Good price.',\n",
       "  'What is not to like about this product.',\n",
       "  'Not bad.',\n",
       "  'Not an issue.',\n",
       "  'Not buggy.',\n",
       "  'Not happy.',\n",
       "  'Not user-friendly.',\n",
       "  'Not good.',\n",
       "  'His behaviour is very bad',\n",
       "  'He is very happy guy',\n",
       "  'This place is very lovely and pet friendly',\n",
       "  'He is very brilliant and sharp in his work'],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open(\"sample_data_train.txt\", \"r+\")\n",
    "f = file.readlines()\n",
    "\n",
    "f1 = list(map(lambda a:a.strip(\"\\n\"), f))\n",
    "\n",
    "f1 = [i for i in f1 if len(i) > 0]\n",
    "\n",
    "\n",
    "X = [i[:-2] for i in f1]\n",
    "Y = [float(i.split(\" \")[-1]) for i in f1]\n",
    "X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96ee25d8-f377-4a68-a544-05b7bb8e9275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'is': 6, 'Not': 6, 'very': 4, 'I': 3, 'not': 3, 'to': 3, 'was': 2, 'has': 2, 'in': 2, 'an': 2, 'Why': 2, 'this': 2, 'so': 2, 'it': 2, 'bad': 2, 'happy': 2, 'He': 2, 'and': 2, 'did': 1, 'get': 1, 'what': 1, 'promised': 1, 'My': 1, 'package': 1, 'arrived': 1, 'yet': 1, 'No': 1, 'one': 1, 'your': 1, 'team': 1, 'been': 1, 'able': 1, 'solve': 1, 'my': 1, 'problem': 1, 'put': 1, 'on': 1, 'hold': 1, 'for': 1, 'hour': 1, 'hard': 1, 'use': 1, 'does': 1, 'fail': 1, 'easily': 1, 'Amazing': 1, 'customer': 1, 'service': 1, 'Love': 1, 'Good': 1, 'price': 1, 'What': 1, 'like': 1, 'about': 1, 'product': 1, 'issue': 1, 'buggy': 1, 'userfriendly': 1, 'good': 1, 'His': 1, 'behaviour': 1, 'guy': 1, 'This': 1, 'place': 1, 'lovely': 1, 'pet': 1, 'friendly': 1, 'brilliant': 1, 'sharp': 1, 'his': 1, 'work': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "vocab = Counter()\n",
    "\n",
    "def clean_doc(X):\n",
    "\n",
    "    # split into tokens by white space\n",
    "    for doc in X:\n",
    "        tokens = doc.split()\n",
    "        # remove punctuation from each token\n",
    "        #tokens = [i.replace(\".\", \"\") for i in tokens]\n",
    "        #tokens = [i.replace(\",\", \"\") for i in tokens]\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        \n",
    "        tokens = [w.translate(table) for w in tokens]\n",
    "        # remove remaining tokens that are not alphabetic\n",
    "        tokens = [word for word in tokens if word.isalpha()]\n",
    "        vocab.update(tokens)\n",
    "        # filter out stop words\n",
    "        #stop_words = set(stopwords.words('english'))\n",
    "        #tokens = [w for w in tokens if not w in stop_words]\n",
    "        # filter out short tokens\n",
    "        #tokens = [word for word in tokens if len(word) > 1]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "a = clean_doc(X)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f0433580-3bde-498f-a010-876deb94c58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = '\\n'.join(vocab)\n",
    "ff = open('vocab.txt', 'w')\n",
    "ff.write(data)\n",
    "ff.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5670f86e-05d3-4a96-896a-a5030b686fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras_preprocessing.text.Tokenizer object at 0x14f0ddf40>\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "# create the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "'''docs = [\"I am a good boy\",\n",
    "        \"he is also a good boy\",\n",
    "        \"they are playing football\",\n",
    "        \"girls are playing kho kho\",\n",
    "        \"good boy is playing football\"]'''\n",
    "\n",
    "tokenizer.fit_on_texts(X)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39f1f480-cb7d-4853-a9fb-61dea0f889fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.125      0.         ... 0.         0.         0.        ]\n",
      " [0.         0.16666667 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.2        ... 0.         0.         0.        ]\n",
      " [0.         0.         0.125      ... 0.         0.         0.        ]\n",
      " [0.         0.         0.11111111 ... 0.11111111 0.11111111 0.11111111]]\n"
     ]
    }
   ],
   "source": [
    "# encode training data set\n",
    "Xtrain = tokenizer.texts_to_matrix(X, mode='freq')\n",
    "print(Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a817e7f-8b83-4913-be3a-c025ee0d463c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-11 19:16:06.513571: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-02-11 19:16:06.514260: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2023-02-11 19:16:07.086599: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2023-02-11 19:16:07.089411: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/1 - 6s - loss: 0.6939 - accuracy: 0.4000\n",
      "Epoch 2/50\n",
      "1/1 - 0s - loss: 0.6924 - accuracy: 0.4000\n",
      "Epoch 3/50\n",
      "1/1 - 0s - loss: 0.6909 - accuracy: 0.4500\n",
      "Epoch 4/50\n",
      "1/1 - 0s - loss: 0.6895 - accuracy: 0.5000\n",
      "Epoch 5/50\n",
      "1/1 - 0s - loss: 0.6880 - accuracy: 0.5500\n",
      "Epoch 6/50\n",
      "1/1 - 0s - loss: 0.6866 - accuracy: 0.5500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-11 19:16:13.017186: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50\n",
      "1/1 - 0s - loss: 0.6852 - accuracy: 0.5500\n",
      "Epoch 8/50\n",
      "1/1 - 0s - loss: 0.6837 - accuracy: 0.6000\n",
      "Epoch 9/50\n",
      "1/1 - 0s - loss: 0.6823 - accuracy: 0.6000\n",
      "Epoch 10/50\n",
      "1/1 - 0s - loss: 0.6808 - accuracy: 0.6500\n",
      "Epoch 11/50\n",
      "1/1 - 0s - loss: 0.6794 - accuracy: 0.7000\n",
      "Epoch 12/50\n",
      "1/1 - 0s - loss: 0.6780 - accuracy: 0.8000\n",
      "Epoch 13/50\n",
      "1/1 - 0s - loss: 0.6766 - accuracy: 0.8000\n",
      "Epoch 14/50\n",
      "1/1 - 0s - loss: 0.6752 - accuracy: 0.8000\n",
      "Epoch 15/50\n",
      "1/1 - 0s - loss: 0.6738 - accuracy: 0.8000\n",
      "Epoch 16/50\n",
      "1/1 - 0s - loss: 0.6724 - accuracy: 0.8000\n",
      "Epoch 17/50\n",
      "1/1 - 0s - loss: 0.6710 - accuracy: 0.9000\n",
      "Epoch 18/50\n",
      "1/1 - 0s - loss: 0.6696 - accuracy: 0.9000\n",
      "Epoch 19/50\n",
      "1/1 - 0s - loss: 0.6682 - accuracy: 0.9000\n",
      "Epoch 20/50\n",
      "1/1 - 0s - loss: 0.6668 - accuracy: 0.9000\n",
      "Epoch 21/50\n",
      "1/1 - 0s - loss: 0.6653 - accuracy: 0.9500\n",
      "Epoch 22/50\n",
      "1/1 - 0s - loss: 0.6639 - accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "1/1 - 0s - loss: 0.6624 - accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "1/1 - 0s - loss: 0.6610 - accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "1/1 - 0s - loss: 0.6595 - accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "1/1 - 0s - loss: 0.6581 - accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "1/1 - 0s - loss: 0.6566 - accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "1/1 - 0s - loss: 0.6551 - accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "1/1 - 0s - loss: 0.6536 - accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "1/1 - 0s - loss: 0.6520 - accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "1/1 - 0s - loss: 0.6505 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "1/1 - 0s - loss: 0.6489 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "1/1 - 0s - loss: 0.6473 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "1/1 - 0s - loss: 0.6457 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "1/1 - 0s - loss: 0.6442 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "1/1 - 0s - loss: 0.6426 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "1/1 - 0s - loss: 0.6409 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "1/1 - 0s - loss: 0.6393 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "1/1 - 0s - loss: 0.6376 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "1/1 - 0s - loss: 0.6359 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "1/1 - 0s - loss: 0.6342 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "1/1 - 0s - loss: 0.6326 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "1/1 - 0s - loss: 0.6309 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "1/1 - 0s - loss: 0.6291 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "1/1 - 0s - loss: 0.6274 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "1/1 - 0s - loss: 0.6256 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "1/1 - 0s - loss: 0.6239 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "1/1 - 0s - loss: 0.6221 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "1/1 - 0s - loss: 0.6203 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "1/1 - 0s - loss: 0.6185 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14386bd90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "#define network\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_shape=(66,), activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile network\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit network\n",
    "model.fit(Xtrain, y = np.array(Y), epochs=50, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "000c2785-da53-4792-8810-a83f74613388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He is not very happy',\n",
       " 'His behaviour is not good',\n",
       " 'lovely',\n",
       " 'bad price',\n",
       " 'He is very userfriendly',\n",
       " 'He is very hard',\n",
       " 'He did not get his price',\n",
       " 'His package is Amazing']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create Test Data\n",
    "\n",
    "file_test = open(\"sample_data_test.txt\", \"r+\")\n",
    "f_test = file_test.readlines()\n",
    "f1_test = list(map(lambda a:a.strip(\"\\n\"), f_test))\n",
    "f1_test = [i for i in f1_test if len(i) > 0]\n",
    "X1 = [i[:-2] for i in f1_test]\n",
    "\n",
    "X_test =[]\n",
    "for doc in X1:\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    tokens = [i.replace(\".\", \"\") for i in tokens]\n",
    "    \n",
    "    tokens = [i.replace(\",\", \"\") for i in tokens]\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "        \n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    #print(\"==\", tokens)\n",
    "    l = [j for j in tokens if j in vocab]\n",
    "    #print(\"=======\", l)\n",
    "    X_test.append(\" \".join(l))\n",
    "        \n",
    "Y_test = [int(i.split(\" \")[-1]) for i in f1_test]\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbabf4c5-0556-4e88-a9f8-0ecb02d3ce83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode training data set\n",
    "Xtest = tokenizer.texts_to_matrix(X_test, mode='count')\n",
    "#print(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66c810ca-2808-48dd-8683-1184691e425f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 62.500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-11 19:17:55.911267: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "loss, acc = model.evaluate(Xtest, np.array(Y_test), verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316f76e2-f2c7-42a5-9c2c-ec9140477072",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
